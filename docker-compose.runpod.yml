version: "3.9"
services:
  train:
    build: .
    image: piper-train:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OMP_NUM_THREADS=1
      - TOKENIZERS_PARALLELISM=false
      - PYTHONUNBUFFERED=1
      # Настройки обучения (при необходимости переопределяйте через env/CLI)
      - CKPT_PATH=/workspace/piper1-gpl/lightning_logs/version_3/checkpoints/epoch=749-step=355500-val_loss=27.5963.ckpt
      - DATA_DIR=/data
      - BATCH_SIZE=24          # пресет для H100: 24/40/64 — варьируйте при запуске
      - MAX_EPOCHS=10000
      - PRECISION=16-mixed     # 16-mixed (cuFFT doesn't support bf16)
      - ACCUM=1                # аккумуляция градиента: 1 (нет), 2 — если надо ужать память
      # S3 sync configuration
      - ENABLE_S3_SYNC=1
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_ENDPOINT_URL=${AWS_ENDPOINT_URL}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-ru-1-hot}
      - S3_BUCKET=${S3_BUCKET}
      - S3_PREFIX=${S3_PREFIX:-piper-training/felix_mirage}
    volumes:
      # Код и логи с чекпоинтами (сохраняем на хосте)
      - .:/workspace/piper1-gpl
      - ./lightning_logs:/workspace/piper1-gpl/lightning_logs
      # Датасет felix_mirage (замапить на хосте на актуальный путь)
      - /media/zudva/git1/git/piper-training/datasets/felix_mirage:/data
    working_dir: /workspace/piper1-gpl
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command: >
      bash -lc "source .venv/bin/activate &&
      python -m piper.train fit \
        --ckpt_path=${CKPT_PATH} \
        --data.config_path=${DATA_DIR}/config.json \
        --data.voice_name=felix_mirage \
        --data.csv_path=${DATA_DIR}/metadata_2col.csv \
        --data.audio_dir=${DATA_DIR}/wavs \
        --data.cache_dir=${DATA_DIR}/.cache \
        --data.espeak_voice=ru \
        --data.batch_size=${BATCH_SIZE} \
        --trainer.max_epochs=${MAX_EPOCHS} \
        --trainer.check_val_every_n_epoch=1 \
        --trainer.strategy=ddp_find_unused_parameters_true \
        --trainer.precision=${PRECISION} \
        --trainer.accumulate_grad_batches=${ACCUM} \
        --trainer.devices=1 \
        --trainer.accelerator=gpu"
    stop_signal: SIGTERM
    stop_grace_period: 90s
